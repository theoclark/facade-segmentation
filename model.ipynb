{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\"\"\"\n","An exploration of the following paper: The devil is in the labels: Semantic segmentation from sentences \n","https://arxiv.org/abs/2202.02002 https://github.com/irfanICMLL/SSIW/tree/master\n","\n","Fine-tuned on the CMP Facade Database: https://cmp.felk.cvut.cz/~tylecr1/facade/\n","\n","\"\"\""],"metadata":{"id":"oFU7n27tcwIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ROOT_PATH = '/content/drive/MyDrive/Colab Notebooks/facade-segmentation/' \n","\n","from google.colab import drive\n","drive.mount('/content/drive') "],"metadata":{"id":"6V-4odh4C_9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download dataset and model weights - only needs to be done once. Alter paths accordingly if root path is changed.\n","!wget https://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_base.zip\n","!unzip /content/CMP_facade_DB_base.zip -d /content/drive/MyDrive/Colab\\ Notebooks/facade-segmentation/Dataset\n","!rm /content/CMP_facade_DB_base.zip\n","!wget https://cloudstor.aarnet.edu.au/plus/s/AtYYaVSVVAlEwve/download -O /content/drive/MyDrive/Colab Notebooks/facade-segmentation/Checkpoints/baseline/segformer_7data.pth\n"],"metadata":{"id":"b7JFXdcJb2Qo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U openmim  \n","!mim install mmcv-full \n","!pip install mmsegmentation timm\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","\n","from abc import ABCMeta, abstractmethod\n","import torch\n","import torch.nn as nn\n","from mmseg.ops import resize\n","import numpy as np\n","from mmcv.cnn import ConvModule, DepthwiseSeparableConvModule\n","from collections import OrderedDict\n","import cv2\n","import random\n","from typing import Optional, Tuple\n","import os\n","import torch.nn.functional as F\n","from functools import partial\n","from mmseg.utils import get_root_logger\n","from mmcv.runner import load_checkpoint\n","import math\n","from mmcv.cnn.bricks import build_norm_layer\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","import logging\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","import clip\n","import matplotlib.pyplot as plt\n","import torch.distributed as dist\n","from torchvision.transforms.functional import resize as im_resize\n","from tqdm import tqdm\n","from PIL import Image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.cuda.set_device(0)\n","dist_url = 'tcp://127.0.0.1:6769'\n","dist_url = dist_url[:-2] + str(os.getpid() % 100).zfill(2)\n","dist.init_process_group(backend=\"nccl\",\n","                    init_method=dist_url,\n","                    world_size=1,\n","                    rank=0,)"],"metadata":{"id":"OFh2fIfXt7Zd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664907057837,"user_tz":-240,"elapsed":35931,"user":{"displayName":"Theo Clark","userId":"15595011225998479017"}},"outputId":"67b3edd5-9d7c-4ac2-f610-bf3e3fe5c395"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: openmim in /usr/local/lib/python3.7/dist-packages (0.3.2)\n","Requirement already satisfied: model-index in /usr/local/lib/python3.7/dist-packages (from openmim) (0.1.11)\n","Requirement already satisfied: rich in /usr/local/lib/python3.7/dist-packages (from openmim) (12.6.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from openmim) (0.4.5)\n","Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.7/dist-packages (from openmim) (21.1.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from openmim) (1.3.5)\n","Requirement already satisfied: Click in /usr/local/lib/python3.7/dist-packages (from openmim) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openmim) (2.23.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from openmim) (0.8.10)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from model-index->openmim) (6.0)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from model-index->openmim) (3.4.1)\n","Requirement already satisfied: ordered-set in /usr/local/lib/python3.7/dist-packages (from model-index->openmim) (4.1.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown->model-index->openmim) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown->model-index->openmim) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown->model-index->openmim) (4.1.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim) (2022.2.1)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim) (1.21.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->openmim) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->openmim) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openmim) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->openmim) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openmim) (2.10)\n","Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich->openmim) (0.9.1)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich->openmim) (2.6.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.openmmlab.com/mmcv/dist/cu113/torch1.12.0/index.html\n","Requirement already satisfied: mmcv-full in /usr/local/lib/python3.7/dist-packages (1.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (1.21.6)\n","Requirement already satisfied: yapf in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (0.32.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (7.1.2)\n","Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (4.6.0.66)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (21.3)\n","Requirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (2.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full) (3.0.9)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mmsegmentation in /usr/local/lib/python3.7/dist-packages (0.28.0)\n","Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.6.11)\n","Requirement already satisfied: mmcls>=0.20.1 in /usr/local/lib/python3.7/dist-packages (from mmsegmentation) (0.24.0)\n","Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from mmsegmentation) (3.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmsegmentation) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmsegmentation) (21.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmsegmentation) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmsegmentation) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mmsegmentation) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mmsegmentation) (1.15.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from timm) (6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->timm) (3.8.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->mmsegmentation) (0.2.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-aub0rg1a\n","  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-aub0rg1a\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n"]}]},{"cell_type":"code","source":["# Classes from the original paper\n","\n","\n","#decode_head.py\n","\n","#from mmcv.cnn import normal_init\n","#from mmcv.runner import auto_fp16, force_fp32\n","\n","#from mmseg.core import build_pixel_sampler\n","#from mmseg.ops import resize\n","#from ..builder import build_loss\n","#from ..losses import accuracy\n","\n","class BaseDecodeHead(nn.Module, metaclass=ABCMeta):\n","    \"\"\"Base class for BaseDecodeHead.\n","    Args:\n","        in_channels (int|Sequence[int]): Input channels.\n","        channels (int): Channels after modules, before conv_seg.\n","        num_classes (int): Number of classes.\n","        dropout_ratio (float): Ratio of dropout layer. Default: 0.1.\n","        conv_cfg (dict|None): Config of conv layers. Default: None.\n","        norm_cfg (dict|None): Config of norm layers. Default: None.\n","        act_cfg (dict): Config of activation layers.\n","            Default: dict(type='ReLU')\n","        in_index (int|Sequence[int]): Input feature index. Default: -1\n","        input_transform (str|None): Transformation type of input features.\n","            Options: 'resize_concat', 'multiple_select', None.\n","            'resize_concat': Multiple feature maps will be resize to the\n","                same size as first one and than concat together.\n","                Usually used in FCN head of HRNet.\n","            'multiple_select': Multiple feature maps will be bundle into\n","                a list and passed into decode head.\n","            None: Only one select feature map is allowed.\n","            Default: None.\n","        loss_decode (dict): Config of decode loss.\n","            Default: dict(type='CrossEntropyLoss').\n","        ignore_index (int | None): The label index to be ignored. When using\n","            masked BCE loss, ignore_index should be set to None. Default: 255\n","        sampler (dict|None): The config of segmentation map sampler.\n","            Default: None.\n","        align_corners (bool): align_corners argument of F.interpolate.\n","            Default: False.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 channels,\n","                 *,\n","                 num_classes,\n","                 dropout_ratio=0.1,\n","                 conv_cfg=None,\n","                 norm_cfg=None,\n","                 act_cfg=dict(type='ReLU'),\n","                 in_index=-1,\n","                 input_transform=None,\n","                 loss_decode=dict(\n","                     type='CrossEntropyLoss',\n","                     use_sigmoid=False,\n","                     loss_weight=1.0),\n","                 ignore_index=255,\n","                 sampler=None,\n","                 align_corners=False):\n","        super(BaseDecodeHead, self).__init__()\n","        self._init_inputs(in_channels, in_index, input_transform)\n","        self.channels = channels\n","        self.num_classes = num_classes\n","        self.dropout_ratio = dropout_ratio\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        self.in_index = in_index\n","        self.loss_decode = None # build_loss(loss_decode)\n","        self.ignore_index = ignore_index\n","        self.align_corners = align_corners\n","        # if sampler is not None:\n","        #     self.sampler = build_pixel_sampler(sampler, context=self)\n","        # else:\n","        #     self.sampler = None\n","        self.sampler = None\n","\n","        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n","        if dropout_ratio > 0:\n","            self.dropout = nn.Dropout2d(dropout_ratio)\n","        else:\n","            self.dropout = None\n","        self.fp16_enabled = False\n","\n","    def extra_repr(self):\n","        pass\n","        # \"\"\"Extra repr.\"\"\"\n","        # s = f'input_transform={self.input_transform}, ' \\\n","        #     f'ignore_index={self.ignore_index}, ' \\\n","        #     f'align_corners={self.align_corners}'\n","        # return s\n","\n","    def _init_inputs(self, in_channels, in_index, input_transform):\n","        # \"\"\"Check and initialize input transforms.\n","        #\n","        # The in_channels, in_index and input_transform must match.\n","        # Specifically, when input_transform is None, only single feature map\n","        # will be selected. So in_channels and in_index must be of type int.\n","        # When input_transform\n","        #\n","        # Args:\n","        #     in_channels (int|Sequence[int]): Input channels.\n","        #     in_index (int|Sequence[int]): Input feature index.\n","        #     input_transform (str|None): Transformation type of input features.\n","        #         Options: 'resize_concat', 'multiple_select', None.\n","        #         'resize_concat': Multiple feature maps will be resize to the\n","        #             same size as first one and than concat together.\n","        #             Usually used in FCN head of HRNet.\n","        #         'multiple_select': Multiple feature maps will be bundle into\n","        #             a list and passed into decode head.\n","        #         None: Only one select feature map is allowed.\n","        # \"\"\"\n","        #\n","        if input_transform is not None:\n","            assert input_transform in ['resize_concat', 'multiple_select']\n","        self.input_transform = input_transform\n","        self.in_index = in_index\n","        if input_transform is not None:\n","            assert isinstance(in_channels, (list, tuple))\n","            assert isinstance(in_index, (list, tuple))\n","            assert len(in_channels) == len(in_index)\n","            if input_transform == 'resize_concat':\n","                self.in_channels = sum(in_channels)\n","            else:\n","                self.in_channels = in_channels\n","        else:\n","            assert isinstance(in_channels, int)\n","            assert isinstance(in_index, int)\n","            self.in_channels = in_channels\n","\n","    def init_weights(self):\n","        pass\n","        # \"\"\"Initialize weights of classification layer.\"\"\"\n","        # normal_init(self.conv_seg, mean=0, std=0.01)\n","\n","    def _transform_inputs(self, inputs):\n","        pass\n","        # \"\"\"Transform inputs for decoder.\n","        #\n","        # Args:\n","        #     inputs (list[Tensor]): List of multi-level img features.\n","        #\n","        # Returns:\n","        #     Tensor: The transformed inputs\n","        # \"\"\"\n","        #\n","        # if self.input_transform == 'resize_concat':\n","        #     inputs = [inputs[i] for i in self.in_index]\n","        #     upsampled_inputs = [\n","        #         resize(\n","        #             input=x,\n","        #             size=inputs[0].shape[2:],\n","        #             mode='bilinear',\n","        #             align_corners=self.align_corners) for x in inputs\n","        #     ]\n","        #     inputs = torch.cat(upsampled_inputs, dim=1)\n","        # elif self.input_transform == 'multiple_select':\n","        #     inputs = [inputs[i] for i in self.in_index]\n","        # else:\n","        #     inputs = inputs[self.in_index]\n","        #\n","        # return inputs\n","\n","    #@auto_fp16()\n","    def forward(self, inputs):\n","        \"\"\"Placeholder of forward function.\"\"\"\n","        pass\n","\n","    def forward_train(self, inputs, img_metas, gt_semantic_seg, train_cfg):\n","        pass\n","        # \"\"\"Forward function for training.\n","        # Args:\n","        #     inputs (list[Tensor]): List of multi-level img features.\n","        #     img_metas (list[dict]): List of image info dict where each dict\n","        #         has: 'img_shape', 'scale_factor', 'flip', and may also contain\n","        #         'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n","        #         For details on the values of these keys see\n","        #         `mmseg/datasets/pipelines/formatting.py:Collect`.\n","        #     gt_semantic_seg (Tensor): Semantic segmentation masks\n","        #         used if the architecture supports semantic segmentation task.\n","        #     train_cfg (dict): The training config.\n","        #\n","        # Returns:\n","        #     dict[str, Tensor]: a dictionary of loss components\n","        # \"\"\"\n","        # seg_logits = self.forward(inputs)\n","        # losses = self.losses(seg_logits, gt_semantic_seg)\n","        # return losses\n","\n","    def forward_test(self, inputs, img_metas, test_cfg):\n","        pass\n","        # \"\"\"Forward function for testing.\n","        #\n","        # Args:\n","        #     inputs (list[Tensor]): List of multi-level img features.\n","        #     img_metas (list[dict]): List of image info dict where each dict\n","        #         has: 'img_shape', 'scale_factor', 'flip', and may also contain\n","        #         'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n","        #         For details on the values of these keys see\n","        #         `mmseg/datasets/pipelines/formatting.py:Collect`.\n","        #     test_cfg (dict): The testing config.\n","        #\n","        # Returns:\n","        #     Tensor: Output segmentation map.\n","        # \"\"\"\n","        # output = self.forward(inputs)\n","        # output = self.emb2cls(output)\n","        # return output\n","\n","    def cls_seg(self, feat):\n","        \"\"\"Classify each pixel.\"\"\"\n","        if self.dropout is not None:\n","            feat = self.dropout(feat)\n","        output = self.conv_seg(feat)\n","        return output\n","\n","    def emb2cls(self, cls_score):\n","        pass\n","        # if hasattr(self.loss_decode, 'vec'):\n","        #     # normalize\n","        #     vec = self.loss_decode.vec.to(device=cls_score.device)\n","        #     if hasattr(self.loss_decode, 'norm'):\n","        #         cls_score = cls_score / cls_score.norm(dim=1, keepdim=True)\n","        #         vec = vec / vec.norm(dim=1, keepdim=True)\n","        #     if hasattr(self.loss_decode, 'logit_scale'):\n","        #         logit_scale = self.loss_decode.logit_scale\n","        #         cls_score = logit_scale * cls_score.permute(0, 2, 3, 1) @ vec.t()  # [N, H, W, num_cls]\n","        #     else:\n","        #         cls_score = cls_score.permute(0, 2, 3, 1) @ vec.t()  # [N, H, W, num_cls]\n","        #     cls_score = cls_score.permute(0, 3, 1, 2)  # [N, num_cls, H, W]\n","        #     return cls_score\n","        # else:\n","        #     raise NameError(\"No vec in loss_decode\")\n","\n","    #@force_fp32(apply_to=('seg_logit', ))\n","    def losses(self, seg_logit, seg_label):\n","        pass\n","        # \"\"\"Compute segmentation loss.\"\"\"\n","        # loss = dict()\n","        # seg_logit = resize(\n","        #     input=seg_logit,\n","        #     size=seg_label.shape[2:],\n","        #     mode='bilinear',\n","        #     align_corners=self.align_corners)\n","        # if self.sampler is not None:\n","        #     seg_weight = self.sampler.sample(seg_logit, seg_label)\n","        # else:\n","        #     seg_weight = None\n","        # seg_label = seg_label.squeeze(1)\n","        # loss['loss_seg'] = self.loss_decode(\n","        #     seg_logit,\n","        #     seg_label,\n","        #     weight=seg_weight,\n","        #     ignore_index=self.ignore_index)\n","        # print(seg_logit.shape, )\n","        # seg_logit = self.emb2cls(seg_logit)\n","        # loss['acc_seg'] = accuracy(seg_logit, seg_label)\n","        # return loss\n","# psp_head.py\n","\n","class PPM(nn.ModuleList):\n","    \"\"\"Pooling Pyramid Module used in PSPNet.\n","    Args:\n","        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n","            Module.\n","        in_channels (int): Input channels.\n","        channels (int): Channels after modules, before conv_seg.\n","        conv_cfg (dict|None): Config of conv layers.\n","        norm_cfg (dict|None): Config of norm layers.\n","        act_cfg (dict): Config of activation layers.\n","        align_corners (bool): align_corners argument of F.interpolate.\n","    \"\"\"\n","\n","    def __init__(self, pool_scales, in_channels, channels, conv_cfg, norm_cfg,\n","                 act_cfg, align_corners):\n","        super(PPM, self).__init__()\n","        self.pool_scales = pool_scales\n","        self.align_corners = align_corners\n","        self.in_channels = in_channels\n","        self.channels = channels\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        for pool_scale in pool_scales:\n","            self.append(\n","                nn.Sequential(\n","                    nn.AdaptiveAvgPool2d(pool_scale),\n","                    ConvModule(\n","                        self.in_channels,\n","                        self.channels,\n","                        1,\n","                        conv_cfg=self.conv_cfg,\n","                        norm_cfg=self.norm_cfg,\n","                        act_cfg=self.act_cfg)))\n","\n","    def forward(self, x):\n","        \"\"\"Forward function.\"\"\"\n","        ppm_outs = []\n","        for ppm in self:\n","            ppm_out = ppm(x)\n","            upsampled_ppm_out = resize(\n","                ppm_out,\n","                size=x.size()[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","            ppm_outs.append(upsampled_ppm_out)\n","        return ppm_outs\n","\n","\n","#@HEADS.register_module()\n","class PSPHead(BaseDecodeHead):\n","    \"\"\"Pyramid Scene Parsing Network.\n","    This head is the implementation of\n","    `PSPNet <https://arxiv.org/abs/1612.01105>`_.\n","    Args:\n","        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n","            Module. Default: (1, 2, 3, 6).\n","    \"\"\"\n","\n","    def __init__(self, pool_scales=(1, 2, 3, 6), **kwargs):\n","        super(PSPHead, self).__init__(**kwargs)\n","        assert isinstance(pool_scales, (list, tuple))\n","        self.pool_scales = pool_scales\n","        self.psp_modules = PPM(\n","            self.pool_scales,\n","            self.in_channels,\n","            self.channels,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg,\n","            align_corners=self.align_corners)\n","        self.bottleneck = ConvModule(\n","            self.in_channels + len(pool_scales) * self.channels,\n","            self.channels,\n","            3,\n","            padding=1,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","        x = self._transform_inputs(inputs)\n","        psp_outs = [x]\n","        psp_outs.extend(self.psp_modules(x))\n","        psp_outs = torch.cat(psp_outs, dim=1)\n","        output = self.bottleneck(psp_outs)\n","        output = self.cls_seg(output)\n","        return output\n","\n","# uper_head.py\n","\n","#@HEADS.register_module()\n","class UPerHead(BaseDecodeHead):\n","    \"\"\"Unified Perceptual Parsing for Scene Understanding.\n","    This head is the implementation of `UPerNet\n","    <https://arxiv.org/abs/1807.10221>`_.\n","    Args:\n","        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n","            Module applied on the last feature. Default: (1, 2, 3, 6).\n","    \"\"\"\n","\n","    def __init__(self, pool_scales=(1, 2, 3, 6), **kwargs):\n","        super(UPerHead, self).__init__(\n","            input_transform='multiple_select', **kwargs)\n","        # PSP Module\n","        self.psp_modules = PPM(\n","            pool_scales,\n","            self.in_channels[-1],\n","            self.channels,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg,\n","            align_corners=self.align_corners)\n","        self.bottleneck = ConvModule(\n","            self.in_channels[-1] + len(pool_scales) * self.channels,\n","            self.channels,\n","            3,\n","            padding=1,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","        # FPN Module\n","        self.lateral_convs = nn.ModuleList()\n","        self.fpn_convs = nn.ModuleList()\n","        for in_channels in self.in_channels[:-1]:  # skip the top layer\n","            l_conv = ConvModule(\n","                in_channels,\n","                self.channels,\n","                1,\n","                conv_cfg=self.conv_cfg,\n","                norm_cfg=self.norm_cfg,\n","                act_cfg=self.act_cfg,\n","                inplace=False)\n","            fpn_conv = ConvModule(\n","                self.channels,\n","                self.channels,\n","                3,\n","                padding=1,\n","                conv_cfg=self.conv_cfg,\n","                norm_cfg=self.norm_cfg,\n","                act_cfg=self.act_cfg,\n","                inplace=False)\n","            self.lateral_convs.append(l_conv)\n","            self.fpn_convs.append(fpn_conv)\n","\n","        self.fpn_bottleneck = ConvModule(\n","            len(self.in_channels) * self.channels,\n","            self.channels,\n","            3,\n","            padding=1,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","\n","    def psp_forward(self, inputs):\n","        \"\"\"Forward function of PSP module.\"\"\"\n","        x = inputs[-1]\n","        psp_outs = [x]\n","        psp_outs.extend(self.psp_modules(x))\n","        psp_outs = torch.cat(psp_outs, dim=1)\n","        output = self.bottleneck(psp_outs)\n","\n","        return output\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","\n","        inputs = self._transform_inputs(inputs)\n","\n","        # build laterals\n","        laterals = [\n","            lateral_conv(inputs[i])\n","            for i, lateral_conv in enumerate(self.lateral_convs)\n","        ]\n","\n","        laterals.append(self.psp_forward(inputs))\n","\n","        # build top-down path\n","        used_backbone_levels = len(laterals)\n","        for i in range(used_backbone_levels - 1, 0, -1):\n","            prev_shape = laterals[i - 1].shape[2:]\n","            laterals[i - 1] += resize(\n","                laterals[i],\n","                size=prev_shape,\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","\n","        # build outputs\n","        fpn_outs = [\n","            self.fpn_convs[i](laterals[i])\n","            for i in range(used_backbone_levels - 1)\n","        ]\n","        # append psp feature\n","        fpn_outs.append(laterals[-1])\n","\n","        for i in range(used_backbone_levels - 1, 0, -1):\n","            fpn_outs[i] = resize(\n","                fpn_outs[i],\n","                size=fpn_outs[0].shape[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","        fpn_outs = torch.cat(fpn_outs, dim=1)\n","        output = self.fpn_bottleneck(fpn_outs)\n","        output = self.cls_seg(output)\n","        return output\n","\n","# segformer_head.py\n","\n","class MLP(nn.Module):\n","    \"\"\"\n","    Linear Embedding\n","    \"\"\"\n","    def __init__(self, input_dim=2048, embed_dim=768):\n","        super().__init__()\n","        self.proj = nn.Linear(input_dim, embed_dim)\n","\n","    def forward(self, x):\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.proj(x)\n","        return x\n","\n","\n","\n","class SegFormerHead(BaseDecodeHead):\n","    \"\"\"\n","    SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n","    \"\"\"\n","    def __init__(self, feature_strides, **kwargs):\n","        super(SegFormerHead, self).__init__(input_transform='multiple_select', **kwargs)\n","        assert len(feature_strides) == len(self.in_channels)\n","        assert min(feature_strides) == feature_strides[0]\n","        self.feature_strides = feature_strides\n","\n","        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.in_channels\n","\n","        decoder_params = dict(embed_dim=768)#kwargs['decoder_params']\n","        embedding_dim = decoder_params['embed_dim']\n","\n","        self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)\n","        self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)\n","        self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)\n","        self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)\n","\n","        self.linear_fuse = ConvModule(\n","            in_channels=embedding_dim*4,\n","            out_channels=embedding_dim,\n","            kernel_size=1,\n","            norm_cfg=dict(type='SyncBN', requires_grad=True)\n","        )\n","\n","        self.linear_pred = nn.Conv2d(embedding_dim, self.num_classes, kernel_size=1)\n","\n","    def forward(self, inputs):\n","        x = inputs #self._transform_inputs(inputs)  # len=4, 1/4,1/8,1/16,1/32\n","        c1, c2, c3, c4 = x\n","\n","        ############## MLP decoder on C1-C4 ###########\n","        n, _, h, w = c4.shape\n","\n","        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n","        _c4 = resize(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","        \n","\n","        _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n","        _c3 = resize(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","        _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n","        _c2 = resize(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n","\n","        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n","\n","        x = self.dropout(_c)\n","\n","        x = self.linear_pred(x)\n","\n","        return x\n","\n","# fpn_head.py\n","\n","#@HEADS.register_module()\n","class FPNHead(BaseDecodeHead):\n","    \"\"\"Panoptic Feature Pyramid Networks.\n","    This head is the implementation of `Semantic FPN\n","    <https://arxiv.org/abs/1901.02446>`_.\n","    Args:\n","        feature_strides (tuple[int]): The strides for input feature maps.\n","            stack_lateral. All strides suppose to be power of 2. The first\n","            one is of largest resolution.\n","    \"\"\"\n","\n","    def __init__(self, feature_strides, **kwargs):\n","        super(FPNHead, self).__init__(\n","            input_transform='multiple_select', **kwargs)\n","        assert len(feature_strides) == len(self.in_channels)\n","        assert min(feature_strides) == feature_strides[0]\n","        self.feature_strides = feature_strides\n","\n","        self.scale_heads = nn.ModuleList()\n","        for i in range(len(feature_strides)):\n","            head_length = max(\n","                1,\n","                int(np.log2(feature_strides[i]) - np.log2(feature_strides[0])))\n","            scale_head = []\n","            for k in range(head_length):\n","                scale_head.append(\n","                    ConvModule(\n","                        self.in_channels[i] if k == 0 else self.channels,\n","                        self.channels,\n","                        3,\n","                        padding=1,\n","                        conv_cfg=self.conv_cfg,\n","                        norm_cfg=self.norm_cfg,\n","                        act_cfg=self.act_cfg))\n","                if feature_strides[i] != feature_strides[0]:\n","                    scale_head.append(\n","                        nn.Upsample(\n","                            scale_factor=2,\n","                            mode='bilinear',\n","                            align_corners=self.align_corners))\n","            self.scale_heads.append(nn.Sequential(*scale_head))\n","\n","    def forward(self, inputs):\n","\n","        x = self._transform_inputs(inputs)\n","\n","        output = self.scale_heads[0](x[0])\n","        for i in range(1, len(self.feature_strides)):\n","            # non inplace\n","            output = output + resize(\n","                self.scale_heads[i](x[i]),\n","                size=output.shape[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","\n","        output = self.cls_seg(output)\n","        return output\n","\n","#fcn_head.py\n","\n","#@HEADS.register_module()\n","class FCNHead(BaseDecodeHead):\n","    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n","    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n","    Args:\n","        num_convs (int): Number of convs in the head. Default: 2.\n","        kernel_size (int): The kernel size for convs in the head. Default: 3.\n","        concat_input (bool): Whether concat the input and output of convs\n","            before classification layer.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 num_convs=2,\n","                 kernel_size=3,\n","                 concat_input=True,\n","                 **kwargs):\n","        assert num_convs >= 0\n","        self.num_convs = num_convs\n","        self.concat_input = concat_input\n","        self.kernel_size = kernel_size\n","        super(FCNHead, self).__init__(**kwargs)\n","        if num_convs == 0:\n","            assert self.in_channels == self.channels\n","\n","        convs = []\n","        convs.append(\n","            ConvModule(\n","                self.in_channels,\n","                self.channels,\n","                kernel_size=kernel_size,\n","                padding=kernel_size // 2,\n","                conv_cfg=self.conv_cfg,\n","                norm_cfg=self.norm_cfg,\n","                act_cfg=self.act_cfg))\n","        for i in range(num_convs - 1):\n","            convs.append(\n","                ConvModule(\n","                    self.channels,\n","                    self.channels,\n","                    kernel_size=kernel_size,\n","                    padding=kernel_size // 2,\n","                    conv_cfg=self.conv_cfg,\n","                    norm_cfg=self.norm_cfg,\n","                    act_cfg=self.act_cfg))\n","        if num_convs == 0:\n","            self.convs = nn.Identity()\n","        else:\n","            self.convs = nn.Sequential(*convs)\n","        if self.concat_input:\n","            self.conv_cat = ConvModule(\n","                self.in_channels + self.channels,\n","                self.channels,\n","                kernel_size=kernel_size,\n","                padding=kernel_size // 2,\n","                conv_cfg=self.conv_cfg,\n","                norm_cfg=self.norm_cfg,\n","                act_cfg=self.act_cfg)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","        x = self._transform_inputs(inputs)\n","        output = self.convs(x)\n","        if self.concat_input:\n","            output = self.conv_cat(torch.cat([x, output], dim=1))\n","        output = self.cls_seg(output)\n","        return output\n","\n","#aspp_head.py\n","\n","class ASPPModule(nn.ModuleList):\n","    \"\"\"Atrous Spatial Pyramid Pooling (ASPP) Module.\n","    Args:\n","        dilations (tuple[int]): Dilation rate of each layer.\n","        in_channels (int): Input channels.\n","        channels (int): Channels after modules, before conv_seg.\n","        conv_cfg (dict|None): Config of conv layers.\n","        norm_cfg (dict|None): Config of norm layers.\n","        act_cfg (dict): Config of activation layers.\n","    \"\"\"\n","\n","    def __init__(self, dilations, in_channels, channels, conv_cfg, norm_cfg,\n","                 act_cfg):\n","        super(ASPPModule, self).__init__()\n","        self.dilations = dilations\n","        self.in_channels = in_channels\n","        self.channels = channels\n","        self.conv_cfg = conv_cfg\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        for dilation in dilations:\n","            self.append(\n","                ConvModule(\n","                    self.in_channels,\n","                    self.channels,\n","                    1 if dilation == 1 else 3,\n","                    dilation=dilation,\n","                    padding=0 if dilation == 1 else dilation,\n","                    conv_cfg=self.conv_cfg,\n","                    norm_cfg=self.norm_cfg,\n","                    act_cfg=self.act_cfg))\n","\n","    def forward(self, x):\n","        \"\"\"Forward function.\"\"\"\n","        aspp_outs = []\n","        for aspp_module in self:\n","            aspp_outs.append(aspp_module(x))\n","\n","        return aspp_outs\n","\n","\n","#@HEADS.register_module()\n","class ASPPHead(BaseDecodeHead):\n","    \"\"\"Rethinking Atrous Convolution for Semantic Image Segmentation.\n","    This head is the implementation of `DeepLabV3\n","    <https://arxiv.org/abs/1706.05587>`_.\n","    Args:\n","        dilations (tuple[int]): Dilation rates for ASPP module.\n","            Default: (1, 6, 12, 18).\n","    \"\"\"\n","\n","    def __init__(self, dilations=(1, 6, 12, 18), **kwargs):\n","        super(ASPPHead, self).__init__(**kwargs)\n","        assert isinstance(dilations, (list, tuple))\n","        self.dilations = dilations\n","        self.image_pool = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            ConvModule(\n","                self.in_channels,\n","                self.channels,\n","                1,\n","                conv_cfg=self.conv_cfg,\n","                norm_cfg=self.norm_cfg,\n","                act_cfg=self.act_cfg))\n","        self.aspp_modules = ASPPModule(\n","            dilations,\n","            self.in_channels,\n","            self.channels,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","        self.bottleneck = ConvModule(\n","            (len(dilations) + 1) * self.channels,\n","            self.channels,\n","            3,\n","            padding=1,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","        x = self._transform_inputs(inputs)\n","        aspp_outs = [\n","            resize(\n","                self.image_pool(x),\n","                size=x.size()[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","        ]\n","        aspp_outs.extend(self.aspp_modules(x))\n","        aspp_outs = torch.cat(aspp_outs, dim=1)\n","        output = self.bottleneck(aspp_outs)\n","        output = self.cls_seg(output)\n","        return output\n","\n","# transform_utils.py\n","\n","\n","\n","def get_imagenet_mean_std() -> Tuple[Tuple[float,float,float], Tuple[float,float,float]]:\n","    \"\"\" See use here in Pytorch ImageNet script: \n","        https://github.com/pytorch/examples/blob/master/imagenet/main.py#L197\n","        Returns:\n","        -   mean: Tuple[float,float,float], \n","        -   std: Tuple[float,float,float] = None\n","    \"\"\"\n","    value_scale = 255\n","    mean = [0.485, 0.456, 0.406]\n","    mean = [item * value_scale for item in mean]\n","    std = [0.229, 0.224, 0.225]\n","    std = [item * value_scale for item in std]\n","    return mean, std\n","            \n","\n","def normalize_img(  input: torch.Tensor, \n","                    mean: Tuple[float,float,float], \n","                    std: Optional[Tuple[float,float,float]] = None):\n","    \"\"\" Pass in by reference Torch tensor, and normalize its values.\n","        Args:\n","        -   input: Torch tensor of shape (3,M,N), must be in this order, and\n","                of type float (necessary).\n","        -   mean: mean values for each RGB channel\n","        -   std: standard deviation values for each RGB channel\n","        Returns:\n","        -   None\n","    \"\"\"\n","    if std is None:\n","        for t, m in zip(input, mean):\n","            t.sub_(m)\n","    else:\n","        for t, m, s in zip(input, mean, std):\n","            t.sub_(m).div_(s)\n","\n","            \n","def pad_to_crop_sz(\n","    image: np.ndarray,\n","    crop_h: int,\n","    crop_w: int,\n","    mean: Tuple[float,float,float]\n","    ) -> Tuple[np.ndarray,int,int]:\n","    ori_h, ori_w, _ = image.shape\n","    pad_h = max(crop_h - ori_h, 0)\n","    pad_w = max(crop_w - ori_w, 0)\n","    pad_h_half = int(pad_h / 2)\n","    pad_w_half = int(pad_w / 2)\n","    if pad_h > 0 or pad_w > 0:\n","        image = cv2.copyMakeBorder(\n","            src=image,\n","            top=pad_h_half,\n","            bottom=pad_h - pad_h_half,\n","            left=pad_w_half,\n","            right=pad_w - pad_w_half,\n","            borderType=cv2.BORDER_CONSTANT,\n","            value=mean)\n","    return image, pad_h_half, pad_w_half\n","\n","\n","def resize_by_scaled_short_side(\n","    image: np.ndarray,\n","    base_size: int,\n","    scale: float) -> np.ndarray:\n","    \"\"\" Equivalent to ResizeShort(), but functional, instead of OOP paradigm, and w/ scale param.\n","\n","\tArgs:\n","\t    image: Numpy array of shape ()\n","\t    scale: scaling factor for image\n","\n","\tReturns:\n","\t    image_scaled:\n","    \"\"\"\n","    h, w, _ = image.shape\n","    short_size = round(scale * base_size)\n","    new_h = short_size\n","    new_w = short_size\n","    # Preserve the aspect ratio\n","    if h > w:\n","        new_h = round(short_size / float(w) * h)\n","    else:\n","        new_w = round(short_size / float(h) * w)\n","    image_scaled = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n","    return image_scaled\n","\n","# segformer.py\n","\n","logger = logging.getLogger(__name__)\n","\n","class FCNHead(nn.Module):\n","    \"\"\"Fully Convolution Networks for Semantic Segmentation.\n","    This head is implemented of `FCNNet <https://arxiv.org/abs/1411.4038>`_.\n","    Args:\n","        num_convs (int): Number of convs in the head. Default: 2.\n","        kernel_size (int): The kernel size for convs in the head. Default: 3.\n","        concat_input (bool): Whether concat the input and output of convs\n","            before classification layer.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 num_convs=1,\n","                 kernel_size=3,\n","                 in_channels=320,\n","                 num_classes=150,\n","                 norm_cfg=None,\n","                 act_cfg=dict(type='ReLU'),\n","                 **kwargs):\n","        assert num_convs >= 0\n","        self.kernel_size = kernel_size\n","        super(FCNHead, self).__init__()\n","        self.in_channels = in_channels\n","        self.channels = 256\n","        self.num_classes = num_classes\n","        self.norm_cfg = norm_cfg\n","        self.act_cfg = act_cfg\n","        convs = []\n","        convs.append(\n","            ConvModule(\n","                self.in_channels,\n","                self.channels,\n","                kernel_size=kernel_size,\n","                padding=kernel_size // 2,  #\n","                conv_cfg=None,\n","                norm_cfg=self.norm_cfg, # sync bn\n","                act_cfg=self.act_cfg)) # relu\n","\n","        self.convs = nn.Sequential(*convs)\n","        self.cls_seg = nn.Conv2d(in_channels=self.channels, out_channels=self.num_classes,\n","                                 kernel_size=1)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","        x = inputs[-2]\n","        output = self.convs(x)\n","        output = self.cls_seg(output)\n","        return output\n","\n","class DepthwiseSeparableASPPModule(ASPPModule):\n","    \"\"\"Atrous Spatial Pyramid Pooling (ASPP) Module with depthwise separable\n","    conv.\"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(DepthwiseSeparableASPPModule, self).__init__(**kwargs)\n","        for i, dilation in enumerate(self.dilations):\n","            if dilation > 1:\n","                self[i] = DepthwiseSeparableConvModule(\n","                    self.in_channels,\n","                    self.channels,\n","                    3,\n","                    dilation=dilation,\n","                    padding=dilation,\n","                    norm_cfg=self.norm_cfg,\n","                    act_cfg=self.act_cfg)\n","\n","\n","class DynHead(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 num_classes,\n","                 norm_cfg,\n","                 act_cfg,\n","                 upsample_f,\n","                 dyn_ch,\n","                 mask_ch,\n","                 use_low_level_info=False,\n","                 channel_reduce_factor=2,\n","                 zero_init=False,\n","                 supress_std=True):\n","        super(DynHead, self).__init__()\n","\n","        channels = dyn_ch\n","        num_bases = 0\n","        if use_low_level_info:\n","            num_bases = mask_ch\n","        num_out_channel = (2 + num_bases) * channels + \\\n","                          channels + \\\n","                          channels * channels + \\\n","                          channels + \\\n","                          channels * num_classes + \\\n","                          num_classes\n","\n","        self.classifier = nn.Sequential(\n","            ConvModule(\n","                in_channels,\n","                in_channels // channel_reduce_factor,\n","                3,\n","                padding=1,\n","                norm_cfg=norm_cfg,\n","                act_cfg=act_cfg, ),\n","            nn.Conv2d(in_channels // channel_reduce_factor, num_out_channel, 1)\n","        )\n","\n","\n","        if zero_init:\n","            nn.init.constant_(self.classifier[-1].weight, 0)\n","        else:\n","            nn.init.xavier_normal_(self.classifier[-1].weight)\n","            if supress_std:\n","                param = self.classifier[-1].weight / num_out_channel\n","                self.classifier[-1].weight = nn.Parameter(param)\n","        nn.init.constant_(self.classifier[-1].bias, 0)\n","\n","    def forward(self, feature):\n","        return self.classifier(feature)\n","\n","\n","#@HEADS.register_module()\n","class BilinearPADHead_fast_xavier_init(ASPPHead):\n","    \"\"\"Encoder-Decoder with Atrous Separable Convolution for Semantic Image\n","    Segmentation.\n","    This head is the implementation of `DeepLabV3+\n","    <https://arxiv.org/abs/1802.02611>`_.\n","    Args:\n","        c1_in_channels (int): The input channels of c1 decoder. If is 0,\n","            the no decoder will be used.\n","        c1_channels (int): The intermediate channels of c1 decoder.\n","    \"\"\"\n","\n","    def __init__(self, c1_in_channels, c1_channels,\n","                 upsample_factor,\n","                 dyn_branch_ch,\n","                 mask_head_ch,\n","                 pad_out_channel_factor=4,\n","                 channel_reduce_factor=2,\n","                 zero_init=False,\n","                 supress_std=True,\n","                 feature_strides=None,\n","                 **kwargs):\n","        super(BilinearPADHead_fast_xavier_init, self).__init__(**kwargs)\n","        assert c1_in_channels >= 0\n","        self.pad_out_channel = self.num_classes\n","        self.upsample_f = upsample_factor\n","        self.dyn_ch = dyn_branch_ch\n","        self.mask_ch = mask_head_ch\n","        self.use_low_level_info = True\n","        self.channel_reduce_factor = channel_reduce_factor\n","\n","        self.aspp_modules = DepthwiseSeparableASPPModule(\n","            dilations=self.dilations,\n","            in_channels=self.in_channels,\n","            channels=self.channels,\n","            conv_cfg=self.conv_cfg,\n","            norm_cfg=self.norm_cfg,\n","            act_cfg=self.act_cfg)\n","\n","        last_stage_ch = self.channels\n","        self.classifier = DynHead(last_stage_ch,\n","                                  self.pad_out_channel,\n","                                  self.norm_cfg,\n","                                  self.act_cfg,\n","                                  self.upsample_f,\n","                                  self.dyn_ch,\n","                                  self.mask_ch,\n","                                  self.use_low_level_info,\n","                                  self.channel_reduce_factor,\n","                                  zero_init,\n","                                  supress_std)\n","\n","        if c1_in_channels > 0:\n","            self.c1_bottleneck = nn.Sequential(\n","                ConvModule(\n","                    c1_in_channels,\n","                    c1_channels,\n","                    3,\n","                    padding=1,\n","                    conv_cfg=self.conv_cfg,\n","                    norm_cfg=self.norm_cfg,\n","                    act_cfg=self.act_cfg),\n","                ConvModule(\n","                    c1_channels,\n","                    self.mask_ch,\n","                    1,\n","                    conv_cfg=self.conv_cfg,\n","                    act_cfg=None,\n","                ),\n","            )\n","        else:\n","            self.c1_bottleneck = None\n","\n","        _, norm = build_norm_layer(self.norm_cfg, 2 + self.mask_ch)\n","        self.add_module(\"cat_norm\", norm)\n","        nn.init.constant_(self.cat_norm.weight, 1)\n","        nn.init.constant_(self.cat_norm.bias, 0)\n","\n","        coord_tmp = self.computer_locations_per_level(640, 640)\n","        self.register_buffer(\"coord\", coord_tmp.float(), persistent=False)\n","\n","    def computer_locations_per_level(self, height, width, h=8, w=8):\n","        shifts_x = torch.arange(0, 1, step=1/w, dtype=torch.float32)\n","        shifts_y = torch.arange(0, 1, step=1/h, dtype=torch.float32)\n","        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n","        locations = torch.stack((shift_x, shift_y), dim=0)\n","        stride_h = height // 32\n","        stride_w = width // 32\n","        coord = locations.repeat(stride_h*stride_w, 1, 1, 1)\n","        return coord\n","\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward function.\"\"\"\n","        # inputs: [1/32 stage, 1/4 stage]\n","        x = inputs[0] # 1/32 stage\n","\n","        aspp_outs = [\n","            resize(\n","                self.image_pool(x),\n","                size=x.size()[2:],\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","        ]\n","        aspp_outs.extend(self.aspp_modules(x))\n","        aspp_outs = torch.cat(aspp_outs, dim=1)\n","        output = self.bottleneck(aspp_outs)\n","\n","        plot = False\n","\n","        if self.c1_bottleneck is not None:\n","            c1_output = self.c1_bottleneck(inputs[1])\n","            if plot:\n","                output2 = output\n","                output3 = c1_output\n","        if self.upsample_f != 8:\n","            c1_output = resize(\n","                c1_output,\n","                scale_factor=self.upsample_f // 8,\n","                mode='bilinear',\n","                align_corners=self.align_corners)\n","        output = self.classifier(output)\n","        output = self.interpolate_fast(output, c1_output, self.cat_norm)\n","        if plot:\n","            outputs = []\n","            outputs.append(output)\n","            outputs.append(output2)\n","            outputs.append(output3)\n","            return outputs\n","\n","        return output\n","\n","    def interpolate(self, x, x_cat=None, norm=None):\n","        dy_ch = self.dyn_ch\n","        B, conv_ch, H, W = x.size()\n","        x = x.view(B, conv_ch, H * W).permute(0, 2, 1)\n","        x = x.reshape(B * H * W, conv_ch)\n","        weights, biases = self.get_subnetworks_params(x, channels=dy_ch)\n","        f = self.upsample_f\n","        self.coord_generator(H, W)\n","        coord = self.coord.reshape(1, H, W, 2, f, f).permute(0, 3, 1, 4, 2, 5).reshape(1, 2, H * f, W * f)\n","        coord = coord.repeat(B, 1, 1, 1)\n","        if x_cat is not None:\n","            coord = torch.cat((coord, x_cat), 1)\n","            coord = norm(coord)\n","\n","        B_coord, ch_coord, H_coord, W_coord = coord.size()\n","        coord = coord.reshape(B_coord, ch_coord, H, f, W, f).permute(0, 2, 4, 1, 3, 5).reshape(1,\n","                                                                                               B_coord * H * W * ch_coord,\n","                                                                                               f, f)\n","        output = self.subnetworks_forward(coord, weights, biases, B * H * W)\n","        output = output.reshape(B, H, W, self.pad_out_channel, f, f).permute(0, 3, 1, 4, 2, 5)\n","        output = output.reshape(B, self.pad_out_channel, H * f, W * f)\n","        return output\n","\n","    def interpolate_fast(self, x, x_cat=None, norm=None):\n","        dy_ch = self.dyn_ch\n","        B, conv_ch, H, W = x.size()\n","        weights, biases = self.get_subnetworks_params_fast(x, channels=dy_ch)\n","        f = self.upsample_f\n","        #self.coord_generator(H, W)\n","        coord = self.coord.reshape(1, H, W, 2, f, f).permute(0, 3, 1, 4, 2, 5).reshape(1, 2, H * f, W * f)\n","        coord = coord.repeat(B, 1, 1, 1)\n","        if x_cat is not None:\n","            coord = torch.cat((coord, x_cat), 1)\n","            coord = norm(coord)\n","\n","        output = self.subnetworks_forward_fast(coord, weights, biases, B * H * W)\n","        return output\n","\n","    def get_subnetworks_params(self, attns, num_bases=0, channels=16):\n","        assert attns.dim() == 2\n","        n_inst = attns.size(0)\n","        if self.use_low_level_info:\n","            num_bases = self.mask_ch\n","        else:\n","            num_bases = 0\n","\n","        w0, b0, w1, b1, w2, b2 = torch.split_with_sizes(attns, [\n","            (2 + num_bases) * channels, channels,\n","            channels * channels, channels,\n","            channels * self.pad_out_channel, self.pad_out_channel\n","        ], dim=1)\n","\n","        w0 = w0.reshape(n_inst * channels, 2 + num_bases, 1, 1)\n","        b0 = b0.reshape(n_inst * channels)\n","        w1 = w1.reshape(n_inst * channels, channels, 1, 1)\n","        b1 = b1.reshape(n_inst * channels)\n","        w2 = w2.reshape(n_inst * self.pad_out_channel, channels, 1, 1)\n","        b2 = b2.reshape(n_inst * self.pad_out_channel)\n","\n","        return [w0, w1, w2], [b0, b1, b2]\n","\n","    def get_subnetworks_params_fast(self, attns, num_bases=0, channels=16):\n","        assert attns.dim() == 4\n","        B, conv_ch, H, W = attns.size()\n","        if self.use_low_level_info:\n","            num_bases = self.mask_ch\n","        else:\n","            num_bases = 0\n","\n","        w0, b0, w1, b1, w2, b2 = torch.split_with_sizes(attns, [\n","            (2 + num_bases) * channels, channels,\n","            channels * channels, channels,\n","            channels * self.pad_out_channel, self.pad_out_channel\n","        ], dim=1)\n","\n","        w0 = resize(w0, scale_factor=self.upsample_f, mode='nearest')\n","        b0 = resize(b0, scale_factor=self.upsample_f, mode='nearest')\n","        w1 = resize(w1, scale_factor=self.upsample_f, mode='nearest')\n","        b1 = resize(b1, scale_factor=self.upsample_f, mode='nearest')\n","        w2 = resize(w2, scale_factor=self.upsample_f, mode='nearest')\n","        b2 = resize(b2, scale_factor=self.upsample_f, mode='nearest')\n","\n","        return [w0, w1, w2], [b0, b1, b2]\n","\n","    def subnetworks_forward(self, inputs, weights, biases, n_subnets):\n","        assert inputs.dim() == 4\n","        n_layer = len(weights)\n","        x = inputs\n","        # NOTE: x has to be treated as min_batch size 1\n","        for i, (w, b) in enumerate(zip(weights, biases)):\n","            x = F.conv2d(\n","                x, w, bias=b,\n","                stride=1, padding=0,\n","                groups=n_subnets\n","            )\n","            if i < n_layer - 1:\n","                x = F.relu(x)\n","        return x\n","\n","    def subnetworks_forward_fast(self, inputs, weights, biases, n_subnets):\n","        assert inputs.dim() == 4\n","        n_layer = len(weights)\n","        x = inputs\n","        if self.use_low_level_info:\n","            num_bases = self.mask_ch\n","        else:\n","            num_bases = 0\n","        for i, (w, b) in enumerate(zip(weights, biases)):\n","            if i == 0:\n","                x = self.padconv(x, w, b, cin=2 + num_bases, cout=self.dyn_ch, relu=True)\n","            if i == 1:\n","                x = self.padconv(x, w, b, cin=self.dyn_ch, cout=self.dyn_ch, relu=True)\n","            if i == 2:\n","                x = self.padconv(x, w, b, cin=self.dyn_ch, cout=self.pad_out_channel, relu=False)\n","        return x\n","\n","    def padconv(self, input, w, b, cin, cout, relu):\n","        input = input.repeat(1, cout, 1, 1)\n","        x = input * w\n","        conv_w = torch.ones((cout, cin, 1, 1), device=input.device)\n","        x = F.conv2d(\n","            x, conv_w, stride=1, padding=0,\n","            groups=cout\n","        )\n","        x = x + b\n","        if relu:\n","            x = F.relu(x)\n","        return x\n","\n","    def coord_generator(self, height, width):\n","        f = self.upsample_f\n","        coord = compute_locations_per_level(f, f)\n","        H = height\n","        W = width\n","        coord = coord.repeat(H * W, 1, 1, 1)\n","        self.coord = coord.to(device='cuda')\n","\n","\n","def compute_locations_per_level(h, w):\n","    shifts_x = torch.arange(\n","        0, 1, step=1 / w,\n","        dtype=torch.float32, device='cuda'\n","    )\n","    shifts_y = torch.arange(\n","        0, 1, step=1 / h,\n","        dtype=torch.float32, device='cuda'\n","    )\n","    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n","    locations = torch.stack((shift_x, shift_y), dim=0)\n","    return locations\n","\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.dwconv = DWConv(hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = self.fc1(x)\n","        x = self.dwconv(x, H, W)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n","        super().__init__()\n","        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n","\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        self.sr_ratio = sr_ratio\n","        if sr_ratio > 1:\n","            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n","            self.norm = nn.LayerNorm(dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","\n","        if self.sr_ratio > 1:\n","            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n","            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n","            x_ = self.norm(x_)\n","            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        else:\n","            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        k, v = kv[0], kv[1]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim,\n","            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n","        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n","\n","        return x\n","\n","\n","class OverlapPatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n","        self.num_patches = self.H * self.W\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n","                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.proj(x)\n","        _, _, H, W = x.shape\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","\n","        return x, H, W\n","\n","\n","class MixVisionTransformer(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n","                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n","                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n","                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.depths = depths\n","\n","        # patch_embed\n","        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n","                                              embed_dim=embed_dims[0]) # 1/4\n","        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n","                                              embed_dim=embed_dims[1]) # 1/8\n","        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n","                                              embed_dim=embed_dims[2]) # auxilary output\n","        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n","                                              embed_dim=embed_dims[3]) # 1/32\n","\n","        # transformer encoder\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","        cur = 0\n","        self.block1 = nn.ModuleList([Block(\n","            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[0])\n","            for i in range(depths[0])])\n","        self.norm1 = norm_layer(embed_dims[0])\n","\n","        cur += depths[0]\n","        self.block2 = nn.ModuleList([Block(\n","            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[1])\n","            for i in range(depths[1])])\n","        self.norm2 = norm_layer(embed_dims[1])\n","\n","        cur += depths[1]\n","        self.block3 = nn.ModuleList([Block(\n","            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[2])\n","            for i in range(depths[2])])\n","        self.norm3 = norm_layer(embed_dims[2])\n","\n","        cur += depths[2]\n","        self.block4 = nn.ModuleList([Block(\n","            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[3])\n","            for i in range(depths[3])])\n","        self.norm4 = norm_layer(embed_dims[3])\n","\n","        # classification head\n","        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def init_weights(self, pretrained=None):\n","        if isinstance(pretrained, str):\n","            logger = get_root_logger()\n","            load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n","\n","    def reset_drop_path(self, drop_path_rate):\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n","        cur = 0\n","        for i in range(self.depths[0]):\n","            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[0]\n","        for i in range(self.depths[1]):\n","            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[1]\n","        for i in range(self.depths[2]):\n","            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[2]\n","        for i in range(self.depths[3]):\n","            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n","\n","    def freeze_patch_emb(self):\n","        self.patch_embed1.requires_grad = False\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        outs = []\n","\n","        # stage 1\n","        x, H, W = self.patch_embed1(x)\n","        for i, blk in enumerate(self.block1):\n","            x = blk(x, H, W)\n","        x = self.norm1(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 2\n","        x, H, W = self.patch_embed2(x)\n","        for i, blk in enumerate(self.block2):\n","            x = blk(x, H, W)\n","        x = self.norm2(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 3\n","        x, H, W = self.patch_embed3(x)\n","        for i, blk in enumerate(self.block3):\n","            x = blk(x, H, W)\n","        x = self.norm3(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 4\n","        x, H, W = self.patch_embed4(x)\n","        for i, blk in enumerate(self.block4):\n","            x = blk(x, H, W)\n","        x = self.norm4(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        return outs\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        # x = self.head(x)\n","\n","        return x\n","\n","\n","class DWConv(nn.Module):\n","    def __init__(self, dim=768):\n","        super(DWConv, self).__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        x = x.transpose(1, 2).view(B, C, H, W).contiguous()\n","        x = self.dwconv(x)\n","        x = x.flatten(2).transpose(1, 2)\n","\n","        return x\n","\n","\n","\n","#@BACKBONES.register_module()\n","class mit_b0(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b0, self).__init__(\n","            patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","#@BACKBONES.register_module()\n","class mit_b1(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b1, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","#@BACKBONES.register_module()\n","class mit_b2(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b2, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","#@BACKBONES.register_module()\n","class mit_b3(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b3, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","#@BACKBONES.register_module()\n","class mit_b4(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b4, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","#@BACKBONES.register_module()\n","class mit_b5(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b5, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","class SegFormer(nn.Module):\n","    def __init__(self, num_classes, load_imagenet_model, imagenet_ckpt_fpath, **kwargs):\n","        super(SegFormer, self).__init__(**kwargs)\n","\n","        self.encoder = mit_b5()\n","        # self.head = BilinearPADHead_fast_xavier_init(num_classes=num_classes,\n","        #                                                 c1_in_channels=64,\n","        #                                                 c1_channels=48,\n","        #                                                 upsample_factor=8,\n","        #                                                 dyn_branch_ch=16,\n","        #                                                 mask_head_ch=16,\n","        #                                                 pad_out_channel_factor=4,\n","        #                                                 channel_reduce_factor=2,\n","        #                                                 zero_init=False,\n","        #                                                 supress_std=True,\n","        #                                                 feature_strides=None,\n","        #                                                 in_channels=512,\n","        #                                                 channels=512,\n","        #                                                 in_index=3,\n","        #                                                 dilations=(1, 3, 6, 9),\n","        #                                                 dropout_ratio=0.1,\n","        #                                                 norm_cfg=dict(type='SyncBN', requires_grad=True),\n","        #                                                 align_corners=False,)\n","\n","        self.head = SegFormerHead(num_classes=num_classes,\n","                                  in_channels=[64, 128, 320, 512],\n","                                  channels=128,\n","                                  in_index=[0,1,2,3],\n","                                  feature_strides=[4, 8, 16, 32],\n","                                  #decoder_params=dict(embed_dim=768),\n","                                  dropout_ratio=0.1,\n","                                  norm_cfg=dict(type='SyncBN', requires_grad=True),\n","                                  align_corners=False)\n","        self.auxi_net = FCNHead(num_convs=1,\n","                                kernel_size=3,\n","                                concat_input=True,\n","                                in_channels=320,\n","                                num_classes=num_classes,\n","                                norm_cfg=dict(type='SyncBN', requires_grad=True))\n","        self.init_weights(load_imagenet_model, imagenet_ckpt_fpath)\n","\n","    def init_weights(self, load_imagenet_model: bool=False, imagenet_ckpt_fpath: str='') -> None:\n","        \"\"\" For training, we use a model pretrained on ImageNet. Irrelevant at inference.\n","            Args:\n","            -   pretrained_fpath: str representing path to pretrained model\n","            Returns:\n","            -   None\n","        \"\"\"\n","        logger.info('=> init weights from normal distribution')\n","        if not load_imagenet_model:\n","            return\n","        if os.path.isfile(imagenet_ckpt_fpath):\n","            print('===========> loading pretrained model {}'.format(imagenet_ckpt_fpath))\n","            self.encoder.init_weights(pretrained=imagenet_ckpt_fpath)\n","        else:\n","            # logger.info(pretrained)\n","            print('cannot find ImageNet model path, use random initialization')\n","            raise RuntimeError('no pretrained model found at {}'.format(imagenet_ckpt_fpath))\n","\n","    def forward(self, inputs):\n","        h = inputs.size()[2]\n","        w = inputs.size()[3]\n","        x = self.encoder(inputs)\n","        #out = self.head([x[3], x[0]])\n","        out = self.head(x)\n","        auxi_out = self.auxi_net(x)\n","        high_out = F.interpolate(out, size=(h,w), mode='bilinear', align_corners=True)\n","        return high_out, out, auxi_out\n","\n","\n","class SegModel(nn.Module):\n","    def __init__(self, criterions, num_classes, load_imagenet_model, imagenet_ckpt_fpath, **kwargs):\n","        super(SegModel, self).__init__(**kwargs)\n","        self.segmodel = SegFormer(num_classes=num_classes,\n","                                  load_imagenet_model=load_imagenet_model,\n","                                  imagenet_ckpt_fpath=imagenet_ckpt_fpath)\n","        self.criterion = None\n","    def forward(self, inputs, gt=None, label_space=None, others=None):\n","        high_reso, low_reso, auxi_out = self.segmodel(inputs)\n","        return high_reso, None, None\n","\n","def get_seg_model(\n","    criterion: list,\n","    n_classes: int,\n","    load_imagenet_model: bool = False,\n","    imagenet_ckpt_fpath: str = '',\n","    **kwargs\n","    ) -> nn.Module:\n","    model = SegModel(criterions=criterion,\n","                     num_classes=n_classes,\n","                     load_imagenet_model=load_imagenet_model,\n","                     imagenet_ckpt_fpath=imagenet_ckpt_fpath)\n","    assert isinstance(model, nn.Module)\n","    return model\n","\n","def get_configured_segformer(\n","    n_classes: int,\n","    criterion: list,\n","    load_imagenet_model: bool = False,\n","    imagenet_ckpt_fpath: str = '',\n","    ) -> nn.Module:\n","    \"\"\"\n","        Args:\n","        -   n_classes: integer representing number of output classes\n","        -   load_imagenet_model: whether to initialize from ImageNet-pretrained model\n","        -   imagenet_ckpt_fpath: string representing path to file with weights to\n","                initialize model with\n","        Returns:\n","        -   model: HRNet model w/ architecture configured according to model yaml,\n","                and with specified number of classes and weights initialized\n","                (at training, init using imagenet-pretrained model)\n","    \"\"\"\n","\n","    model = get_seg_model(criterion, n_classes, load_imagenet_model, imagenet_ckpt_fpath)\n","    return model\n","\n","# color_seg.py\n","\n","def visual_segments(segments, rgb):\n","    seg = Image.fromarray(segments)\n","    rgb = Image.fromarray(rgb)\n","\n","    seg1 = seg.convert('RGBA')\n","    rgb1 = rgb.convert('RGBA')\n","\n","    vis_seg = Image.blend(rgb1, seg1, 0.8)\n","    return vis_seg\n","\n","def make_palette(num_classes=256):\n","    \"\"\"\n","    Inputs:\n","        num_classes: the number of classes\n","    Outputs:\n","        palette: the colormap as a k x 3 array of RGB colors\n","    \"\"\"\n","    palette = np.zeros((num_classes, 3), dtype=np.uint8)\n","    for k in range(0, num_classes):\n","        label = k\n","        i = 0\n","        while label:\n","            palette[k, 0] |= (((label >> 0) & 1) << (7 - i))\n","            palette[k, 1] |= (((label >> 1) & 1) << (7 - i))\n","            palette[k, 2] |= (((label >> 2) & 1) << (7 - i))\n","            label >>= 3\n","            i += 1\n","    idx1 = np.arange(0, num_classes, 2)[::-1]\n","    idx2 = np.arange(1, num_classes, 2)\n","    idx = np.concatenate([idx1[:, None], idx2[:, None]], axis=1).flatten()\n","    palette = palette[idx]\n","    palette[num_classes - 1, :] = [255, 255, 255]\n","    return palette\n","\n","def color_seg(seg, palette=None):\n","    PALETTE = make_palette(256)\n","    if palette == None:\n","        color_out = PALETTE[seg.reshape(-1)].reshape(seg.shape + (3,))\n","    else:\n","        color_out = palette[seg.reshape(-1)].reshape(seg.shape + (3,))\n","    return color_out"],"metadata":{"id":"VOAWEPGclmnf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom classes and functions\n","\n","class CMP_Dataset(Dataset):\n","  def __init__(self, root_path):\n","    super(CMP_Dataset,self).__init__()\n","    self.base_path = os.path.join(root_path,'base')\n","    self.file_names = os.listdir(self.base_path)\n","    self.pairs = []\n","    for i in range(1,379):\n","      inpt_pth = os.path.join(self.base_path,'cmp_b'+str(i).zfill(4)+'.jpg')\n","      tgt_pth = os.path.join(self.base_path,'cmp_b'+str(i).zfill(4)+'.png')\n","\n","      input = cv2.imread(inpt_pth, -1)[:, :, ::-1]\n","      target = read_image(tgt_pth) - 1\n","\n","      self.pairs.append((input,target))\n","\n","  def __len__(self):\n","    return len(self.pairs)\n","\n","  def __getitem__(self,idx):\n","    return self.pairs[idx]\n","\n","class Decoder(nn.Module):\n","    def __init__(self, num_classes):\n","      super(Decoder,self).__init__()\n","      self.head = SegFormerHead(num_classes=num_classes,\n","                                    in_channels=[64, 128, 320, 512],\n","                                    channels=128,\n","                                    in_index=[0,1,2,3],\n","                                    feature_strides=[4, 8, 16, 32],\n","                                    dropout_ratio=0.1,\n","                                    norm_cfg=dict(type='SyncBN', requires_grad=True),\n","                                    align_corners=False)\n","      \n","    def forward(self, x,h,w):\n","      out = self.head(x)\n","      high_out = F.interpolate(out, size=(h,w), mode='bilinear', align_corners=True)\n","      return high_out, out, None\n","\n","def load_model(root_path, ckpt=None):\n","\n","  encoder = mit_b5()\n","  assert isinstance(encoder, nn.Module)\n","  encoder.eval()\n","  encoder_path = os.path.join(root_path,'Checkpoints/baseline/segformer_7data.pth')\n","  encoder_checkpoint = torch.load(encoder_path, map_location='cpu')['state_dict']\n","  encoder_filter = {k[24:]: v for k, v in encoder_checkpoint.items() if 'module.segmodel.encoder' in k}\n","  encoder.load_state_dict(encoder_filter, strict=True)\n","  encoder.to(device)\n","\n","  decoder = Decoder(512)\n","  assert isinstance(decoder, nn.Module)\n","  decoder.eval()\n","  if ckpt == None:\n","    decoder_path = os.path.join(root_path,'Checkpoints/baseline/segformer_7data.pth')\n","    decoder_checkpoint = torch.load(decoder_path, map_location='cpu')['state_dict']\n","    decoder_filter = {k[16:]: v for k, v in decoder_checkpoint.items() if 'module.segmodel.head' in k}\n","    decoder.load_state_dict(decoder_filter, strict=True)\n","  else:\n","    decoder_path = os.path.join(root_path,'Checkpoints/','decoder_'+ckpt+'.pt')\n","    decoder_checkpoint = torch.load(decoder_path)\n","    decoder.load_state_dict(decoder_checkpoint['decoder_state_dict'])\n","  decoder.to(device)\n","\n","  return encoder, decoder\n","\n","# from original paper (edited)\n","def get_prediction(embs, gt_embs_list):\n","    prediction = []\n","    logits = []\n","    B, _, _, _ = embs.shape\n","    for b in range(B):\n","        score = embs[b,...]\n","        score = score.unsqueeze(0)\n","        emb = gt_embs_list\n","        emb = emb / emb.norm(dim=1, keepdim=True)\n","        score = score / score.norm(dim=1, keepdim=True)\n","        score = score.permute(0, 2, 3, 1) @ emb.t()\n","        # [N, H, W, num_cls] You maybe need to remove the .t() based on the shape of your saved .npy\n","        score = score.permute(0, 3, 1, 2)  # [N, num_cls, H, W]\n","        prediction.append(score.max(1)[1])\n","        logits.append(score)\n","    if len(prediction) == 1:\n","        prediction = prediction[0]\n","        logit = logits[0]\n","    else:\n","        prediction = torch.cat(prediction, dim=0)\n","        logit = torch.cat(logits, dim=0)\n","    return logit\n","\n","# from original paper (edited)\n","def single_scale_single_crop_cuda(encoder,decoder,\n","                      image: np.ndarray,\n","                      h: int, w: int, gt_embs_list,\n","                      args=None) -> np.ndarray:\n","    ori_h, ori_w, _ = image.shape\n","    mean, std = get_imagenet_mean_std()\n","    crop_h = (np.ceil((ori_h - 1) / 32) * 32).astype(np.int32)\n","    crop_w = (np.ceil((ori_w - 1) / 32) * 32).astype(np.int32)\n","    \n","    image, pad_h_half, pad_w_half = pad_to_crop_sz(image, crop_h, crop_w, mean)\n","    image = torch.from_numpy(image.transpose((2, 0, 1))).float()\n","    normalize_img(image, mean, std)\n","    image = image.unsqueeze(0).cuda()\n","    with torch.no_grad():\n","        out = encoder(image)\n","    emb,_,_ = decoder(out,h,w)\n","    logit = get_prediction(emb, gt_embs_list)\n","    prediction_crop = F.softmax(logit * 100, dim=1).squeeze()\n","\n","    # disregard predictions from padded portion of image\n","    prediction_crop = prediction_crop[:, pad_h_half:pad_h_half + ori_h, pad_w_half:pad_w_half + ori_w]\n","\n","    prediction_crop = im_resize(prediction_crop,(h,w))\n","    prediction_crop = prediction_crop.permute(1,2,0)\n","\n","    return prediction_crop\n","\n","def fine_tune(encoder,decoder,data,optimiser,criterion,epochs=1):\n","  encoder.train()\n","  decoder.train()\n","  for param in encoder.parameters():\n","      param.requires_grad = False\n","  losses = []\n","  for epoch in range(epochs):\n","    total_loss = 0\n","    count = 0\n","    for (input, target) in tqdm(data):\n","        if input.shape[0]*input.shape[1] < 750000:      # to fit on the GPU\n","          count += 1\n","          h, w, _ = input.shape\n","          input = resize_by_scaled_short_side(input, 720, 1)\n","          pred = single_scale_single_crop_cuda(encoder,decoder, input, h, w, gt_embs_list=text_embs, args=None)\n","          del input\n","          target = target.flatten().cuda()\n","          pred = pred.reshape((-1,12))\n","          optimiser.zero_grad()\n","          loss = criterion(pred,target)\n","          del pred, target\n","          loss.backward()\n","          optimiser.step()\n","          loss = loss.cpu().detach().numpy().item()\n","          total_loss += loss\n","          losses.append(loss)\n","    print(f'Epoch {epoch+1}: loss={total_loss/count}')\n","  plt.plot(losses)\n","\n","def calc_accuracy(data,text_embs,encoder,decoder):\n","  text_embs.requires_grad = False\n","  encoder.eval()\n","  decoder.eval()\n","  accuracy = 0\n","  for (input,target) in tqdm(data):\n","      image_resized = resize_by_scaled_short_side(input, 720, 1)\n","      h, w, _ = input.shape\n","      pred = single_scale_single_crop_cuda(encoder,decoder, image_resized, h, w, gt_embs_list=text_embs, args=None)\n","      prediction = pred.argmax(axis=-1).squeeze().flatten()\n","      target = target.flatten().cuda()\n","      accuracy += torch.where(prediction == target,1,0).sum().cpu().item() / target.shape[0]\n","  accuracy = accuracy / len(data)\n","  print(\"\\n\",accuracy) \n","\n","def save_images(data,encoder,decoder,root_path,sub_dir,n=10):\n","  sub_path = os.path.join(root_path,\"Predictions/\",sub_dir)\n","  if not os.path.exists(sub_path):\n","    os.makedirs(sub_path)\n","  text_embs.requires_grad = False\n","  encoder.eval()\n","  decoder.eval()\n","  accuracy = 0\n","  count = 0\n","  for (input, target) in data:\n","    if count < n:\n","      h, w, _ = input.shape\n","      if h*w < 500000:   # to fit on the gpu\n","        count+=1\n","        image_resized = resize_by_scaled_short_side(input, 720, 1)\n","        pred = single_scale_single_crop_cuda(encoder,decoder, image_resized, h, w, gt_embs_list=text_embs, args=None)\n","        prediction = pred.argmax(axis=-1).squeeze()\n","        prediction = prediction.cpu().numpy()        \n","        pred_color = color_seg(prediction)\n","        vis_seg = visual_segments(pred_color, input)\n","        save_path = os.path.join(sub_path,\"img_\"+str(count)+'.png')\n","        vis_seg.save(save_path)"],"metadata":{"id":"UaTrloNNIKP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use CLIP to calculate new embeddings for CMP Facade Database\n","\n","def compute_embeddings(labels,definitions,root_path,version='v1'):\n","  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","  clip_model, preprocess = clip.load('ViT-B/32', device)\n","  definition_list = []\n","  for i in range(1,13):\n","    definition_list.append(definitions[labels[i]])\n","  text_inputs = torch.cat([clip.tokenize(definition) for definition in definition_list]).to(device)\n","  text_embs = clip_model.encode_text(text_inputs).float()\n","  emb_path = os.path.join(root_path,'Embeddings/embeddings_'+version+'.pt')\n","  torch.save(text_embs,emb_path)\n","\n","labels = {\n","    1:\"background\",\n","    2:\"facade\",\n","    3:\"window\",\n","    4:\"door\",\n","    5:\"cornice\",\n","    6:\"sill\",\n","    7:\"balcony\",\n","    8:\"blind\",\n","    9:\"deco\",\n","    10:\"molding\",\n","    11:\"pillar\",\n","    12:\"shop\",\n","}\n","\n","# Definitions \n","\n","# V1\n","definitions = {\n","    \"background\":\"background\",\n","    \"facade\":\"facade\",\n","    \"window\":\"window\",\n","    \"door\":\"door\",\n","    \"cornice\":\"cornice\",\n","    \"sill\":\"sill\",\n","    \"balcony\":\"balcony\",\n","    \"blind\":\"blind\",\n","    \"deco\":\"deco\",\n","    \"molding\":\"molding\",\n","    \"pillar\":\"pillar\",\n","    \"shop\":\"shop\",\n","}\n","\n","# # V2\n","# definitions = {\n","#     \"background\":\"This is an image of background.\",\n","#     \"facade\":\"This is an image of facade.\",\n","#     \"window\":\"This is an image of window.\",\n","#     \"door\":\"This is an image of door.\",\n","#     \"cornice\":\"This is an image of cornice.\",\n","#     \"sill\":\"This is an image of window sill.\",\n","#     \"balcony\":\"This is an image of balcony.\",\n","#     \"blind\":\"This is an image of blind.\",\n","#     \"deco\":\"This is an image of decoration.\",\n","#     \"molding\":\"This is an image of molding.\",\n","#     \"pillar\":\"This is an image of pillar.\",\n","#     \"shop\":\"This is an image of shop.\",\n","# }\n","\n","# # V3 - taken from wikipedia where possible in line with the original paper\n","# definitions = {             \n","#     \"background\":\"This is an image of background. A background is the part of a picture, scene, or design that forms a setting for the main figures or objects\",\n","#     \"facade\":\"This is an image of facade. A facade is the front part or exterior of a building\",\n","#     \"window\":\"This is an image of window. A window is an opening in a wall, door, roof, or vehicle that allows the exchange of light\",\n","#     \"door\":\"This is an image of door. A door is a movable barrier that allows entry into and out of a building\",\n","#     \"cornice\":\"This is an image of cornice. A cornice is any horizontal decorative moulding that crowns a building\",\n","#     \"sill\":\"This is an image of window sill. A window sill is the horizontal structure at the bottom of a window\",\n","#     \"balcony\":\"This is an image of balcony. A balcony is a platform projecting from the wall of a building\",\n","#     \"blind\":\"This is an image of blind. A blind is a type of window covering\",\n","#     \"deco\":\"This is an image of decoration. A decoration is an object designed to make something more attractive\",\n","#     \"molding\":\"This is an image of molding. A molding is used to cover transitions between surfaces\",\n","#     \"pillar\":\"This is an image of pillar. A pillar is a structural element that transmits, through compression, the weight of the structure above to other structural elements below\",\n","#     \"shop\":\"This is an image of shop. A shop is a commercial establishment or place of business\",\n","# }"],"metadata":{"id":"TfVMfXFdI0tN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate new embeddings using definitions\n","NEW_EMBEDDING_VERSION = 'v1'\n","compute_embeddings(labels,definitions,ROOT_PATH,NEW_EMBEDDING_VERSION)"],"metadata":{"id":"OtIe6s1bfSoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load data\n","dataset_pth = os.path.join(ROOT_PATH,'Dataset')\n","data = CMP_Dataset(dataset_pth)"],"metadata":{"id":"fXezSy_rMn79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load text embeddings\n","EMBEDDING_VERSION = 'v1'\n","emb_path = os.path.join(ROOT_PATH,'Embeddings/embeddings_'+EMBEDDING_VERSION+'.pt')\n","text_embs = torch.load(emb_path)"],"metadata":{"id":"ynpmlDeTDx6U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model from checkpoint\n","CHECK_POINT_VERSION = None                                            # 'None' loads the weights from the original paper\n","encoder,decoder = load_model(ROOT_PATH, ckpt=CHECK_POINT_VERSION)     # Segformer split into encoder and decoder to allow fine tuning of just decoder"],"metadata":{"id":"73v6JqXzAcDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LR = 0.0001\n","optimiser = torch.optim.Adam(decoder.parameters(),lr=LR)\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"0Mo5LMPIITpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 5\n","fine_tune(encoder,decoder,data,optimiser,criterion,epochs=EPOCHS)"],"metadata":{"id":"0v_VBMaQ1PCT","colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"status":"ok","timestamp":1664888340908,"user_tz":-240,"elapsed":1018010,"user":{"displayName":"Theo Clark","userId":"12647340414785662548"}},"outputId":"4cd35cbe-b543-4d52-e763-08bc741cae27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 378/378 [03:20<00:00,  1.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: loss=2.16509505364953\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 378/378 [03:24<00:00,  1.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: loss=2.0637635324059462\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 378/378 [03:24<00:00,  1.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: loss=2.038373613357544\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 378/378 [03:23<00:00,  1.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: loss=2.027704446490218\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 378/378 [03:24<00:00,  1.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: loss=2.0174955193589375\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# Calculate per-pixel accuracy\n","calc_accuracy(data,text_embs,encoder,decoder)         "],"metadata":{"id":"5f4ptfvgyp89","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664909705333,"user_tz":-240,"elapsed":183214,"user":{"displayName":"Theo Clark","userId":"15595011225998479017"}},"outputId":"cc0c700e-0df3-4b15-e69b-6e94098429fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 378/378 [03:03<00:00,  2.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," 0.39644133317962515\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# Save new checkpoint\n","SAVE_NAME = '5x1e'     \n","save_path = os.path.join(ROOT_PATH,'Checkpoints/','decoder_'+SAVE_NAME+'.pt')\n","torch.save({'decoder_state_dict': decoder.state_dict(),}, save_path)"],"metadata":{"id":"HXC7VBak2lTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save sample images with prediction overlay\n","SUB_DIR = \"5x1e\"\n","N_IMAGES = 5\n","save_images(data,encoder,decoder,ROOT_PATH,SUB_DIR,n=N_IMAGES)"],"metadata":{"id":"Jb5M27UkTxrL"},"execution_count":null,"outputs":[]}]}